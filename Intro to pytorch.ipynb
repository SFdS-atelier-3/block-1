{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick intro to `pytorch`\n",
    "\n",
    "This notebook presents a quick intro to the typical building blocks you need to train a paramtrized function via gradient descent\n",
    "\n",
    "## `pytorch` as a tools for learning functions\n",
    "`pytorch` lets you code functions. It comes with very similar primitives and function names as `numpy`, but has two extra things: It can seamlessly switch its location to GPUs and it can compute the gradient of any array manipulation you come up with without any effort on your side.\n",
    "\n",
    "Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(20., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad_fn(torch.ones(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see two things:\n",
    "1. The gradient was computed correctly to 2x\n",
    "2. Even when computing 2x we would get the possibility of having a gradient. Sometimes we don't care and would like to avoid using extra memory. In these cases we can `detach` our tensor from a graph of gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * x.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks as functions that store some parameters\n",
    "\n",
    "As seen above, `pytorch` naturally keeps track of gradients of the operations one does. For parametrized functions such as neural networks it is useful to gather all the parameters that can vary within an object, to have them all together.\n",
    "\n",
    "To make such an object, we can employ a python `class` structure and benefit from that fact that `pytorch` has templates for this, specifically `torch.nn.Module`. In this structure, our code would look something like this:\n",
    "\n",
    "```python\n",
    "class MyParametrizedFunction(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ... all the specifications/parameters it needs ...):\n",
    "        super(MyParametrizedFunction, self).__init__()\n",
    "        # ... do something to initialize the parameters\n",
    "        # ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # all the computations leading from input `x` to\n",
    "        # the function output\n",
    "```\n",
    "\n",
    "Very often, the specifications we input to such a function are shapes and sizes of the arrays of parameters. To be able to keep some order in the `__init__` function, and to avoid unexpected behavior, it is useful to split the storage of specifications and the building of the arrays, making the structure look more like this:\n",
    "\n",
    "```python\n",
    "class MyParametrizedFunction(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ... all the specifications/parameters it needs ...):\n",
    "        super(MyParametrizedFunction, self).__init__()\n",
    "        # store all the inputs/specs/parameters\n",
    "        \n",
    "        self.build()  # call build to add any extra internal structure\n",
    "    \n",
    "    def build(self):\n",
    "        # ... do something to initialize/create the parameters\n",
    "        # ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # all the computations leading from input `x` to\n",
    "        # the function output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's encapsulate a linear function!\n",
    "This exercise is purely pedagogical since `pytorch` already has linear/affine units like this, but let's create one anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction(torch.nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=1, bias=True):\n",
    "        super(LinearFunction, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.bias = bias\n",
    "    \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.matrix = torch.nn.Parameter(torch.randn(self.n_input, self.n_output))\n",
    "        if self.bias:\n",
    "            self.bias_vector = torch.nn.Parameter(torch.randn(self.n_output))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x of shape (batch, n_input)\n",
    "        output = torch.mm(x, self.matrix)\n",
    "        if self.bias:\n",
    "            output = output + self.bias_vector\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LinearFunction(n_input=1, n_output=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on some data spaced between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0, 1, .01)[:, np.newaxis]\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.detach().numpy().ravel(), y.detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely no surprise: It's a linear function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some quick ways of creating a neural network\n",
    "\n",
    "`pytorch` offers the possibility to create virtually any function from its components. Most standard neural networks actually take a very simple form of stacked layers. Creating these has been facilitated in neural network packages in general, and specifically in `pytorch`. Concatenating layers using `torch.nn.Sequential` is one of these facilitations. Let's take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_one_hidden = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a 1-hidden layer neural network of one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.arange(-3, 3, .01)[:, np.newaxis]\n",
    "y2 = net_one_hidden(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x2.detach().numpy().ravel(), y2.detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a bit more interesting by adding some hidden dimensions into which the linear layer can project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_one_hidden_better = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.arange(-1, 1, .01)[:, np.newaxis]\n",
    "y3 = net_one_hidden_better(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x3.detach().numpy().ravel(), y3.detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about more layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_one_hidden_even_better = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = torch.arange(-1, 1, .01)[:, np.newaxis]\n",
    "y4 = net_one_hidden_even_better(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x4.detach().numpy().ravel(), y4.detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 01:** Write a function that takes a series of integers as input, indicating neural network layer sizes, and initializes a neural network with precisely those sizes, and a `ReLU` nonlinearity *between* each layer (don't put one at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pytorch/solution01.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/pytorch/solution01.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = create_neural_network((1, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y5 = network(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x4.detach().numpy().ravel(), y5.detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we get these to fit a function we like?\n",
    "\n",
    "The success of neural networks comes from their ability to fit arbitrary functions given enough data.\n",
    "\n",
    "But we have to give them hints on what we want it to do. We do this by providing examples and also by specifying an error.\n",
    "\n",
    "Let us take a look at how this is done.\n",
    "\n",
    "First we need to know which function we would like to learn. Let's create several, and restrict ourselves to functions living on the interval $(-1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return np.maximum(x, 0)  # relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x, f=2.):\n",
    "    return np.sin(2 * np.pi * f * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-1, 1, 0.001, requires_grad=False)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = f1(x).detach()\n",
    "y2 = f2(x).detach()\n",
    "y3 = f3(x).detach()\n",
    "y4 = f4(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(x.numpy().ravel(), y1.numpy().ravel())\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(x.numpy().ravel(), y2.numpy().ravel())\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(x.numpy().ravel(), y3.numpy().ravel())\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(x.numpy().ravel(), y4.numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have data (`x`) and a target (`y`) let's take a look at how neural networks are generally trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural networks with `pytorch`\n",
    "\n",
    "To train a neural network, we need\n",
    "- a network\n",
    "- data (here `x`)\n",
    "- target (here `y`)\n",
    "- an *optimization criterion* or *loss* or *error function*\n",
    "- an *optimizer* that attempts to minimize the error by gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_neural_network((1, 100, 100, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization criterion\n",
    "The optimization criterion quantifies the similiarity between what the network outputs for the data and what we expect the network to output for the data.\n",
    "\n",
    "With category labels, this is often the *cross-entropy* loss, which we will get to later.\n",
    "\n",
    "For this application here we will use the *mean squared error*:\n",
    "\n",
    "$$MSE(p, y) = \\frac{1}{N}\\|p - y\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(torch.ones(10), torch.ones(10) * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimizer\n",
    "Once we have a loss function and its gradient, we need a mechanism to take steps in order to reduce it. *Stochastic Gradient Descent* is the method of choice here. As a first choice, the optimizer `Adam` is useful to achieve quite good convergence.\n",
    "\n",
    "To instantiate an optimizer, we need to give it the parameters of the network that we want optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(net.parameters())\n",
    "#optimizer = SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One optimization cycle\n",
    "Let's do one round of an optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.from_numpy(np.random.permutation(len(x))[:100]) # to choose some random values\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x[indices]\n",
    "y1_batch = y1[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = net(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(prediction, y1_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have the optimizer take a step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this cycle, let's check the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(net(x_batch), y1_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that it is slightly lower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function for training one epoch\n",
    "An *epoch* is one pass through the dataset. It is useful to write a function that can perform one pass over an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network, data, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x_batch, y_batch in data:\n",
    "        prediction = network(x_batch)\n",
    "        loss = criterion(prediction, y_batch)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A useful iterator for data batches\n",
    "`pytorch` has some built-in objects to make data batches, which we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make a dataset out of our x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = DataLoader(list(zip(x, y1)), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `train_epoch` function\n",
    "Now we can try out the loss function and observe the evolution of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_epoch(net, data1, mse, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = net(x).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), predictions)\n",
    "plt.plot(x.numpy(), y1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that iterates over epochs\n",
    "\n",
    "Usually neural networks need several passes over the data to fully train. We can automate this, and the creation of the optimizer and the criterion, inside another function, which we will call `train_mse_adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mse_adam(network, dataset, n_epochs=10):\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(network.parameters())\n",
    "    \n",
    "    all_losses = []\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        \n",
    "        losses = train_epoch(network, dataset, criterion, optimizer)\n",
    "        all_losses.append(losses)\n",
    "    \n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_neural_network((1, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = train_mse_adam(net, data1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(all_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = net(x).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), prediction1)\n",
    "plt.plot(x.numpy(), y1.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 02:** Train networks for the 3 other functions. How big do they need to be? For y4, does it vary with the sinusoid frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pytorch/solution02.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/pytorch/solution02.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
