{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn: The biggest machine-learning library\n",
    "<img src=\"images/scikit-learn-logo-notext.png\"></img>\n",
    "\n",
    "### has become machine learning ingredient of the \"data-science triad\": `jupyter notebooks`, `pandas`, `scikit-learn`\n",
    "### has \"made machine learning boring\"\n",
    "### history: \n",
    "- 2007 David Cournapeau Google Summer of Code (with Jarrod Millman)\n",
    "- 2010 Parietal Team (Inria Saclay) takes over, first release Feb 1st, 2010\n",
    "- Fabian Pedregosa full-time engineer 2010-2012\n",
    "- Currently Olivier Grisel and Andreas Mueller and several others full-time open source\n",
    "\n",
    "<a href=\"http://scikit-learn.org\" style=\"font-size: 20pt\">Scikit-learn.org</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's jump right in and predict some molecule properties!\n",
    "\n",
    "We will fit an estimator to be able to predict a molecule property from its structure.\n",
    "\n",
    "Let's start by loading some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm7 = np.load(\"./qm7.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, charges, energies = qm7['positions'], qm7['charges'], qm7['energies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data represent atom positions and types for 7165 small organic molecules. From `positions` and `charges` we'd like to be able to predict `energies` using machine learning.\n",
    "\n",
    "### Feature extraction: To be able to use scikit-learn we need `X` and `y`\n",
    "\n",
    "Every supervised algorithm in scikit-learn requires input of `X` and `y` (unsupervised algorithms only need `X`).\n",
    "\n",
    "`X` is \"the data\" and `y` is the prediction target. The goal of the esimator is to be able to predict `y` from `X` as well as possible.\n",
    "\n",
    "`X` is of shape `(n_samples, n_features)` and `y` is of shape `n_samples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for Chemistry: Coulomb matrices predict molecule properties\n",
    "\n",
    "Our `y` is `energies`. Let's set it to be that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = energies\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `X` is more complicated to obtain. We need to construct it from `positions` and `charges`. We can construct Coulomb matrices from them and order their rows and columns with the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coulomb_matrices(positions, charges):\n",
    "    all_atom_distances = np.linalg.norm(positions[:, :, np.newaxis] -\n",
    "                                        positions[:, np.newaxis], axis=-1)\n",
    "    all_charge_products = charges[:, :, np.newaxis] * charges[:, np.newaxis]\n",
    "    non_zero_mask = (all_charge_products != 0) & (all_atom_distances > 0)\n",
    "    coulomb_matrices = np.zeros_like(all_atom_distances)\n",
    "    coulomb_matrices[non_zero_mask] = (all_charge_products[non_zero_mask] / \n",
    "                                       all_atom_distances[non_zero_mask])\n",
    "    coulomb_matrices.reshape(coulomb_matrices.shape[0], -1)[:, ::coulomb_matrices.shape[1] + 1] = charges ** 2.4\n",
    "    return coulomb_matrices\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "def sort_coulomb_matrices(coulomb_matrices, jitter=0., random_state=0):\n",
    "\n",
    "    rng = check_random_state(random_state)\n",
    "    row_norms = np.linalg.norm(coulomb_matrices, axis=2)\n",
    "    jitters = rng.rand(*row_norms.shape) * jitter\n",
    "    row_norms += jitters\n",
    "    \n",
    "    row_order = row_norms.argsort(axis=1)[:, ::-1]\n",
    "    sorted_coulomb_matrices = coulomb_matrices[np.arange(len(coulomb_matrices))[:, np.newaxis, np.newaxis],\n",
    "                                               row_order[:, :, np.newaxis],\n",
    "                                               row_order[:, np.newaxis]]\n",
    "    return sorted_coulomb_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm = sort_coulomb_matrices(compute_coulomb_matrices(positions, charges), jitter=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that these functions give us 7165 matrices of shape 23x23. But what we need is `X` of shape `(7165, something)`. We can obtain this from `scm` by reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scm.reshape(scm.shape[0], -1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data splitting: train-test-split\n",
    "\n",
    "In order to be able to evaluate a model we need to have it issue predictions on data it did not see during training. In order to ensure this, we can use scikit-learn functionality for data splitting. A very simple, straight-forward way of doing this is to use the function `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple regression estimator: Kernel Ridge Regression\n",
    "Now we proceed to creating an estimator object, in this case a `KernelRidge` regression.\n",
    "\n",
    "#### Fitting the regression to data\n",
    "The procedure of fitting it to data and testing it is the same for all estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr = KernelRidge(kernel='laplacian', alpha=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(predictions, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(predictions, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(predictions, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(predictions, ytest, \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those seem like pretty solid scores. For many applications, an `r2` value of `.997` is absolutely great. It turns out though, that the state of the art in error on this data set is an order of magnitude lower than our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the robustness of an estimator: cross-validation\n",
    "\n",
    "Cross-validation is a procedure in which the data are split in multiple ways and the estimator is fit on each split. The scores are gathered in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have a handle on the splits, and we can specify this using the `cv`-argument of this function, to which we pass a cross-validation splitter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=6, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(kr, X, y, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `n_jobs=3` we ask that 3 of these estimations be run in parallel if memory and number of CPUs allows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are negative, because we chose an error measure as the score. Since some selection procedures thinks that scoring is *higher is better*, errors are presented negatively so that a higher value means a lower error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have a score summary of MAE={-np.mean(scores):0.2f}+-{np.std(scores):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a pretty consistent estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating estimators with `pipelines`\n",
    "\n",
    "Sometimes there are preprocessing steps that depend on the data. In order for these steps not to use testing data, and to create a global estimator object, it is often useful to create a `pipeline` that concatenates them all. In our case, the input data is probably quite large in norm. This is generally not good for estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linalg.norm(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(scaler, KernelRidge(kernel='laplacian', gamma=1e-10, alpha=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pipeline.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(p, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating hyperparameter optimization with `GridSearchCV`\n",
    "\n",
    "Our pipeline currently has two hyperparameters that we have set by hand. If we play around with these hyperparameters a lot and evaluate a cross-validation every time, we might end up overfitting the dataset.\n",
    "\n",
    "While there is nothing wrong with setting hyperparameters based on how well they make the estimator predict, the data used for final evaluation should not be used for hyperparameter selection.\n",
    "\n",
    "Scikit-learn provides tools for parameter selection by trainset splitting, for example `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc = GridSearchCV(pipeline, param_grid=dict(\n",
    "                                kernelridge__gamma=[1e-10, 5e-10, 1e-9],\n",
    "                                kernelridge__alpha=[1e-8, 1e-9, 1e-10],),\n",
    "                   scoring=\"neg_mean_absolute_error\",\n",
    "                   cv=3\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gsc.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(ytest, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 01:** Use a `GridSearchCV`-wrapped estimator in a cross-validation loop using `cross_val_score`. What do the resulting scores mean with respect to the estimator without `GridSearchCV`-wrapping?\n",
    "\n",
    "Watch out: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scikit-learn/solution01.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/scikit-learn/solution01.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "### Estimator Objects\n",
    "- An estimator is a class that implements the methods `fit(X, y)` and `predict(X)`. `fit` makes the estimator learn something from the data, `predict` issues predictions for new data.\n",
    "\n",
    "- `X` is always a 2D matrix of shape `n_samples, n_features`.\n",
    "- `y` is either a 1D vector of length `n_samples` or a 2D matrix of shape `n_samples, n_features`\n",
    "\n",
    "- the `__init__` method must only store input parameters\n",
    "\n",
    "\n",
    "Let's make our own `scikit-learn` estimator which runs a pytorch neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(*layer_sizes):\n",
    "    layers = []\n",
    "    for input_size, output_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        layers.append(torch.nn.Linear(input_size, output_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    return torch.nn.Sequential(*layers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network, data, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y in data:\n",
    "        p = network(x)\n",
    "        loss = criterion(p, y)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nll_adam(network, data, n_epochs=10):\n",
    "    all_losses = []\n",
    "    optimizer = torch.optim.Adam(network.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for e in range(n_epochs):\n",
    "        losses = train_epoch(network, data, criterion, optimizer)\n",
    "        all_losses.append(losses)\n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layer_sizes=(), n_epochs=10, batch_size=32):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        u_label, i_label = np.unique(y, return_inverse=True)\n",
    "        self.u_label = u_label\n",
    "        \n",
    "        layers = (n_features,) + self.layer_sizes + (len(u_label),)\n",
    "        \n",
    "        self.network = create_neural_network(*layers)\n",
    "        \n",
    "        data = torch.utils.data.DataLoader(list(zip(X.astype('float32'), i_label)), batch_size=self.batch_size)\n",
    "        train_nll_adam(self.network, data, n_epochs=self.n_epochs)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xtorch = torch.from_numpy(X.astype('float32'))\n",
    "        predicted_scalars = self.network(Xtorch).detach().cpu().numpy()\n",
    "        predicted_labels = predicted_scalars.argmax(1)\n",
    "        return self.u_label[predicted_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 02:** Fit our sklearn-pytorch estimator to some digits data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1\\.** Create a scikit-learn-pytorch neural network using the class above, with `layer_sizes=(100,)` and `n_epochs=100`. Call it `my_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scikit-learn/solution02.1.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/scikit-learn/solution02.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2\\.** import `load_digits` from `sklearn.datasets`. Run this function and store the output in `digits`. Store `digits.data` in `X` and `digits.target` in `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scikit-learn/solution02.2.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/scikit-learn/solution02.2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\\.** Use the `fit` method to fit the neural network to the first 1000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scikit-learn/solution02.3.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/scikit-learn/solution02.3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\.** Use the `predict` to predict on the remaining data points, and check how many digits were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/scikit-learn/solution02.4.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-1/master/solutions/scikit-learn/solution02.4.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
